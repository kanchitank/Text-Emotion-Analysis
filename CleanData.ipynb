{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, sys, os, csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(tweet):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    tweet = pattern.sub(r\"\\1\\1\", tweet)\n",
    "    tweet = re.sub(r'http.?://[^\\s]+[\\s]?', '', tweet)\n",
    "    tweet = re.sub('\\d+', '', tweet)\n",
    "    punct = string.punctuation\n",
    "    trantab = str.maketrans(punct, len(punct) * ' ')  # Every punctuation symbol will be replaced by a space\n",
    "    tweet = tweet.translate(trantab)\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)     \n",
    "        \n",
    "    ps = PorterStemmer()\n",
    "    words = tweet.split()\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in stemmed_words]\n",
    "    tweet = \" \".join(lemma_words)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_prob(word): return dictionary[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower())\n",
    "dictionary = Counter(words(open('dataset/wordlists/merged.txt').read()))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "\n",
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def fix_hashtag(text):\n",
    "    text = text.group().split(\":\")[0]\n",
    "    text = text[1:] # remove '#'\n",
    "    try:\n",
    "        test = int(text[0])\n",
    "        text = text[1:]\n",
    "    except:\n",
    "        pass\n",
    "    output = ' '.join(viterbi_segment(text)[0])\n",
    "    #print(output)\n",
    "    return output\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"(#[A-Za-z0-9]+)\", fix_hashtag, tweet)\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "\"\"\"def remove_stopwords(word_list):\n",
    "        filtered_tweet=\"\"\n",
    "        for word in word_list:\n",
    "            word = word.lower() \n",
    "            if word not in stopwords.words(\"english\"):\n",
    "                filtered_tweet=filtered_tweet + \" \" + word\n",
    "        \n",
    "        return filtered_tweet.lstrip()\"\"\"\n",
    "    \n",
    "def de_repeat(tweet):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", tweet)\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "def remove_urls(tweet):\n",
    "    return re.sub(r'http.?://[^\\s]+[\\s]?', '', tweet)\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    # Make translation table\n",
    "    punct = string.punctuation\n",
    "    trantab = str.maketrans(punct, len(punct) * ' ')  # Every punctuation symbol will be replaced by a space\n",
    "    return tweet.translate(trantab)\n",
    "\n",
    "def whitespaces(tweet):\n",
    "    tweet = tweet.strip()\n",
    "    return tweet\n",
    "\n",
    "def emoticon_punct(tweet):\n",
    "    tweet = re.sub('<[^>]*.,!>', '', tweet)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', tweet)\n",
    "    for emot in EMOTICONS:\n",
    "        tweet = re.sub(u'(' + emot + ')', \"_\".join(EMOTICONS[emot].replace(\",\", \" \").split()), tweet)\n",
    "    return tweet\n",
    "   \n",
    "def emoji_toword(tweet):\n",
    "    for emot in UNICODE_EMO:\n",
    "        tweet = tweet.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \" \").replace(\":\", \" \").split()))\n",
    "    return tweet\n",
    "\n",
    "def remove_digits(tweet):\n",
    "    return re.sub('\\d+', '', tweet)\n",
    "\n",
    "def to_lower(tweet):\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = tweet.split()\n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n",
    "    return \" \".join(clean_words)\n",
    "    \n",
    "def stemming(tweet):\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)     \n",
    "        \n",
    "    ps = PorterStemmer()\n",
    "    words = tweet.split()\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in stemmed_words]\n",
    "    #porter = PorterStemmer()\n",
    "    #words = input_text.split()\n",
    "    #stemmed_words = [porter.stem(word) for word in words]\n",
    "    return \" \".join(lemma_words)\n",
    "       \n",
    "def vectorise_label(label):\n",
    "    if label == \"empty\":return 1 # neutral\n",
    "    elif label == \"sadness\":return 2 # sad\n",
    "    elif label == \"enthusiasm\":return 3 # happy\n",
    "    elif label == \"neutral\":return 0 # neutral\n",
    "    elif label == \"worry\":return 4 # sad\n",
    "    elif label == \"surprise\":return 5 # happy\n",
    "    elif label == \"love\":return 6 # love\n",
    "    elif label == \"fun\":return 7 # happy\n",
    "    elif label == \"hate\":return 8 # anger\n",
    "    elif label == \"happiness\":return 9 # happy\n",
    "    elif label == \"boredom\":return 10 # neutral\n",
    "    elif label == \"relief\":return 11 # happy\n",
    "    elif label == \"anger\":return 12 #anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (40000, 4)\n",
      "empty : @tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('dataset/data/text_emotion.csv', sep=',')\n",
    "print(\"Dataset shape:\",data_train.shape)\n",
    "print(data_train.sentiment[0],\":\",data_train.content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  100 \n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "dataWriter = csv.writer(open('cleaned_data/data.csv', 'w'), delimiter=',',lineterminator=\"\\n\")\n",
    "\n",
    "total = 40000\n",
    "for i in range(40000):\n",
    "    #print(\"Progress: \",round(i/total*100,2),\"   \",end=\"\\r\")\n",
    "    tweet= clean_tweet(data_train.content[i])\n",
    "    #tweet = remove_stopwords(tweet.split())\n",
    "    dataWriter.writerow([tweet, str(vectorise_label(data_train.sentiment[i]))])\n",
    "    #sys.stdout.write(\"\\033[F\")\n",
    "    \n",
    "print(\"Progress: \",100,\"\\nComplete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dataWriter = csv.writer(open('emotion_data_cleaned.csv', 'w', encoding = \"utf-8\"), delimiter=',',lineterminator=\"\\n\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "total = 55773\n",
    "for i in range(55773):\n",
    "    tweet = prep(data_train.iloc[:,0][i]) \n",
    "    #tweet = remove_mentions(data_train.iloc[:,-1][i]) \n",
    "    #tweet = remove_urls(data_train.iloc[:,-1][i])\n",
    "    #tweet = remove_digits(data_train.content[i])\n",
    "    #tweet = remove_punctuation(data_train.content[i])\n",
    "    #tweet = emoji_toword(data_train.content[i])\n",
    "    #tweet = emoticon_punct(data_train.content[i])\n",
    "    #tweet = to_lower(data_train.content[i])\n",
    "    #tweet = whitespaces(data_train.content[i])\n",
    "    #tweet = stemming(data_train.content[i])\n",
    "    #tweet = remove_stopwords(data_train.content[i])\n",
    "    \n",
    "    dataWriter.writerow([tweet])\n",
    "    #sys.stdout.write(\"\\033[F\")\n",
    "    \n",
    "print(\"Progress: \",100,\"\\nComplete!\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open('cleaned_data/data.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
